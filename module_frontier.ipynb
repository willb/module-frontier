{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following problem:  you'd like to enable users to automatically extract a function from a Jupyter notebook and publish it as a service.  Actually serializing a closure from a function in a given environment is not difficult, given the [`cloudpickle`](https://github.com/cloudpipe/cloudpickle) module.  But merely constructing a serialized closure isn't enough, since in general this function may require other modules to be available to run in another context.  \n",
    "\n",
    "Therefore, we need some way to identify the modules required by a function (and, ideally, the packages that provide these modules).  Since engineering time is limited, it's probably better to have an optimistic-and-imperfect estimate and allow users to override it (by supplying additional dependencies when they publish a function) than it is to have a sound and conservative module list.\n",
    "\n",
    "The [`modulefinder`](https://docs.python.org/3.6/library/modulefinder.html) module in the Python standard library might initially seem like an attractive option, but it is unsuitable because it operates at the level of scripts.  In order to use `modulefinder` on a single function from a notebook, we'd either have an imprecise module list (due to running the whole notebook) or we'd need to essentially duplicate a lot of its effort in order to [slice backwards](https://en.wikipedia.org/wiki/Program_slicing) from the function invocation so we could extract a suitably pruned script.\n",
    "\n",
    "Fortunately, you can interrogate nearly any aspect of any object in a Python program, and functions are no exception.  If we could inspect the captured variables in a closure, we could identify the ones that are functions and figure out which modules they were declared in.  That would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def module_frontier(f):\n",
    "  worklist = [f]\n",
    "  seen = set()\n",
    "  mods = set()\n",
    "  for fn in worklist:\n",
    "    cvs = inspect.getclosurevars(fn)\n",
    "    gvars = cvs.globals\n",
    "    for k, v in gvars.items():\n",
    "      if inspect.ismodule(v):\n",
    "        mods.add(v.__name__)\n",
    "      elif inspect.isfunction(v) and id(v) not in seen:\n",
    "        seen.add(id(v))\n",
    "        mods.add(v.__module__)\n",
    "        worklist.append(v)\n",
    "      elif hasattr(v, \"__module__\"):\n",
    "        mods.add(v.__module__)\n",
    "  return list(mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`inspect`](https://docs.python.org/3/library/inspect.html#inspect.Signature.bind) module provides a friendly interface to inspecting object metadata.  In the above function, we're constructing a worklist of all of the captured variables in a given function's closure.  We're then constructing a set of all of the modules directly or transitively referred to by those captured variables, whether these are modules referred to directly, modules declaring functions referred to by captured variables, or modules declaring other values referred to by captured variables (e.g., native functions).  Note that we add any functions we find to the worklist (although we don't handle `eval` or other techniques), so we'll capture at least some of the transitive call graph in this case.\n",
    "\n",
    "This approach seems to work pretty sensibly on simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "\n",
    "def f(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "def g(a, b):\n",
    "    return dot(a, b)\n",
    "\n",
    "def h(a, b):\n",
    "    return f(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': ['numpy.core.multiarray', 'numpy'],\n",
       " 'g': ['numpy.core.multiarray'],\n",
       " 'h': ['numpy.core.multiarray', '__main__', 'numpy']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k.__name__ : module_frontier(k) for k in [f,g,h]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works on itself, which is a relief:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inspect']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_frontier(module_frontier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these initial experiments are promising, we shouldn't expect that a simple approach won't cover everything we might want to do.  Let's look at a (slightly) more involved example to see if it breaks down.\n",
    "\n",
    "We'll use the k-means clustering implementation from scikit-learn to optimize some cluster centers in a model object.  We'll then capture that model object in a closure and analyze it to see what we might need to import to run it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(1000, 2)\n",
    "\n",
    "model = KMeans(random_state=0).fit(data)\n",
    "\n",
    "def km_predict_one(sample):\n",
    "    sample = np.array(sample).reshape(1,-1)\n",
    "    return model.predict(sample)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_predict_one([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn.cluster.k_means_', 'numpy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_frontier(km_predict_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good.  Let's say we want to publish this simple model as a lighter-weight service (without a scikit-learn dependency).  We can get that by reimplementing the `predict` method from the k-means model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = model.cluster_centers_\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def km_predict_two(sample):\n",
    "    _, idx = min([(norm(sample - center), idx) for idx, center in enumerate(centers)])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_predict_two([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get if we analyze the second method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_frontier(km_predict_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This is a problem!_  We'd expect that `norm` would be a captured variable in the body of `km_predict_two` (and thus that `numpy.linalg` would be listed in its module frontier), but that isn't the case.  We can inspect the closure variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClosureVars(nonlocals={}, globals={'centers': array([[ 0.84764105,  0.15126092],\n",
       "       [ 0.76039891,  0.84267179],\n",
       "       [ 0.14871645,  0.48878765],\n",
       "       [ 0.504071  ,  0.19950246],\n",
       "       [ 0.24086058,  0.83960717],\n",
       "       [ 0.17965483,  0.14435561],\n",
       "       [ 0.85896443,  0.48130862],\n",
       "       [ 0.51427112,  0.58713655]])}, builtins={'min': <built-in function min>, 'enumerate': <class 'enumerate'>}, unbound=set())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getclosurevars(km_predict_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cluster centers as well as the `min` function and the `enumerate` type.  But `norm` isn't in the list.  Let's dive deeper.  We can use the [`dis`](https://docs.python.org/3/library/dis.html) module to inspect the Python bytecode for a given function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: LOAD_GLOBAL(min)\n",
      "2: LOAD_CLOSURE(sample)\n",
      "4: BUILD_TUPLE()\n",
      "6: LOAD_CONST(<code object <listcomp> at 0x1137fdd20, file \"<ipython-input-8-5a350184a257>\", line 5>)\n",
      "8: LOAD_CONST('km_predict_two.<locals>.<listcomp>')\n",
      "10: MAKE_FUNCTION()\n",
      "12: LOAD_GLOBAL(enumerate)\n",
      "14: LOAD_GLOBAL(centers)\n",
      "16: CALL_FUNCTION()\n",
      "18: GET_ITER()\n",
      "20: CALL_FUNCTION()\n",
      "22: CALL_FUNCTION()\n",
      "24: UNPACK_SEQUENCE()\n",
      "26: STORE_FAST(_)\n",
      "28: STORE_FAST(idx)\n",
      "30: LOAD_FAST(idx)\n",
      "32: RETURN_VALUE()\n"
     ]
    }
   ],
   "source": [
    "from dis import Bytecode\n",
    "for inst in Bytecode(km_predict_two):\n",
    "    print(\"%d: %s(%s)\" % (inst.offset, inst.opname, inst.argrepr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ha!  The body of our list comprehension, which contains the call to `norm`, is a separate code object that has been stored in a constant.  Let's look at the constants for our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " <code object <listcomp> at 0x1137fdd20, file \"<ipython-input-8-5a350184a257>\", line 5>,\n",
       " 'km_predict_two.<locals>.<listcomp>')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_predict_two.__code__.co_consts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the code object in the constant list and use `dis` to disassemble it as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: BUILD_LIST()\n",
      "2: LOAD_FAST(.0)\n",
      "4: FOR_ITER(to 30)\n",
      "6: UNPACK_SEQUENCE()\n",
      "8: STORE_FAST(idx)\n",
      "10: STORE_FAST(center)\n",
      "12: LOAD_GLOBAL(norm)\n",
      "14: LOAD_DEREF(sample)\n",
      "16: LOAD_FAST(center)\n",
      "18: BINARY_SUBTRACT()\n",
      "20: CALL_FUNCTION()\n",
      "22: LOAD_FAST(idx)\n",
      "24: BUILD_TUPLE()\n",
      "26: LIST_APPEND()\n",
      "28: JUMP_ABSOLUTE()\n",
      "30: RETURN_VALUE()\n"
     ]
    }
   ],
   "source": [
    "for inst in Bytecode(km_predict_two.__code__.co_consts[1]):\n",
    "    print(\"%d: %s(%s)\" % (inst.offset, inst.opname, inst.argrepr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done so, we can see that the list comprehension has loaded `norm` from a global, which we can then resolve and inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.linalg.linalg.norm>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_predict_two.__globals__[\"norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numpy.linalg.linalg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.__module__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ongoing work and future notebooks will cover refinements to this technique.  It has been fun to learn about the power (and limitations) of the `inspect` module -- as well as a little more about how Python handles code blocks in list comprehension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
